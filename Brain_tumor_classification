import zipfile
import os
import shutil
import random

# Unzip the dataset
zip_path = '/content/archive (1).zip'
extract_path = '/content/'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Paths
yes_dir = os.path.join(extract_path, 'yes')
no_dir = os.path.join(extract_path, 'no')

# Create train and validation directories
train_dir = os.path.join(extract_path, 'train')
val_dir = os.path.join(extract_path, 'val')

os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)

for category in ['yes', 'no']:
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(val_dir, category), exist_ok=True)

# Split data into train and validation sets
def split_data(source_dir, train_dir, val_dir, split_ratio=0.8):
    files = os.listdir(source_dir)
    random.shuffle(files)
    split_point = int(len(files) * split_ratio)
    
    train_files = files[:split_point]
    val_files = files[split_point:]
    
    for file in train_files:
        shutil.move(os.path.join(source_dir, file), os.path.join(train_dir, os.path.basename(source_dir), file))
    
    for file in val_files:
        shutil.move(os.path.join(source_dir, file), os.path.join(val_dir, os.path.basename(source_dir), file))

# Split the data
split_data(yes_dir, train_dir, val_dir)
split_data(no_dir, train_dir, val_dir)

# Check the structure
print(f"Train Yes: {len(os.listdir(os.path.join(train_dir, 'yes')))} images")
print(f"Train No: {len(os.listdir(os.path.join(train_dir, 'no')))} images")
print(f"Validation Yes: {len(os.listdir(os.path.join(val_dir, 'yes')))} images")
print(f"Validation No: {len(os.listdir(os.path.join(val_dir, 'no')))} images")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Image data generator for preprocessing
datagen = ImageDataGenerator(rescale=1.0/255.0)

train_generator = datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

# Load the VGG16 model
vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model
vgg16_base.trainable = False

# Add custom top layers
vgg16_model = Sequential([
    vgg16_base,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compile the model
vgg16_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history_vgg16 = vgg16_model.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator
)

vgg16_model.save('D:/Poojan/Machine Learning/Dataset/Brain_tumor.h5')
